MSE:
El loss se calcula y la red se actualiza solo para una acción (la acción escogida en la transicion (s,a,r,d,s_)

Env:
    Continuar hasta que deje de mejorar.*
    IR GUARDANDO RESULTADOS; X; Y; Z;*

Entrenamiento:
    Graficas: media cada ~100 para qe se veamejor la evolucion del loss y los rewards *
    Batch normalization?¿ *
    MSE? yo creo que está bien -> MAE (PROBAR)

Validacion:
    -10000 Imagenes
    -Cargar directorio (funciones que me paso brais)*
    (Implementar en env, env que reciba una lista en vez del array )*

    -Cuando se prediga una clase incorrecta, anotar la confianza del predictor en la clases maxima*
    -Calcular tambien aciertos*


-17/02 18:12

Modificar net?añadir otra capa de conv parra que quede 5*5 (o 3*3)*
Nunca elije la accion 3*
MSE/MAE??
Con Mean absolute error y TD 0.00001 parece "menos estable" pero alcanza mejores resultados (0.84)
con MSE y td 0.00001 parece mas estable

El loss se va porlas nubes!!
Los pesos explotan porque no hay negativos
Se devuelve un ultimoi poaso como si fuera el penultimo dando lugar a resultados incorrectos
¡REWARDS INTERMEDIOS!->mejora desde el anterior paso
REWARD FINAL:mejora desde el paso inicial

Desde la última reunión:
    Mean Squared Error:
        Me dijisteis que lo cambiara por CategoricalCrossEntropy. Sin embargo, como la red está aproximando (como en una regresión) el Q value del estado y la acción elegidos
    no parece que tenga sentido ya que aproximamos un numero real y no unas probabilidades. también me di cuenta de que la última capa de la red tenía una funcion de activación
    softmax, lo cual tampoco tenía sentido, y que fue eliminada.
    Finalización según resultados.
        La implementé para probar de forma que finalizara despues de un step con resultados peores que el anterior. Los pesos explotaban y los q values crecían cada vez más.
    Sin embargo aún debería hacer mas pruebas, finalizando despues de varios steps empeorando, y tambien con rewards intermedios para que aprenda de cada step directamente
    Loss: he probado Mean Absolute Error. en un caso concreto he conseguido resultados buenos pero no se han repetido
    He vueltto a MSE con la idea de repetir los últimos resultados buenos de antes de la reunión. Al final he cambiado a Huber Loss
    Step size: lo mismo,con 32 he conseguido resultados buenos pero en casos especificos
    Con finalizacion a la primera acción que empeora: Los pesos explotan (falta probar con mas pasos despues de que empeore)
    Con rewards en cada paso en lugar de al final todavía no he conseguido resultados buenos

    He implementado el env con el generator y la validacion con 1000 imagenes en lugar de 25
    He implementado rewards intermedios
    He implementado finalizacion por acción que empeora.
        Problemas:
            En test no sabriamos como finalizar
                Idea: implementar otro tipo de finalización (por la certeza de los primeros resultados o algo asi)
            Solo resultados positivos, los pesos explotan, no aprende bien
                Idea: probar con rewards intermedios para que aprenda de cada acción
    He implementado sistema de logs para poder ejecutar scripts con varias tareas y luego ver los resultados
    He implementado reinforce(Policy Gradient con baseline) (pero aun no lo he probado)
    He solucionado problemas de memoria
        En WindowEnvGenerator el reescalado y preprocesado delas imagenges pasa de ser tensorflow a Pillow. PArece que resuelve problema de memoria pero podría ir más lento (no lo he medido)
        En WindowEnvGenerator y en DeepQLearning, llamar a predict_on_batch en lugar de predict (y gc.collect)

